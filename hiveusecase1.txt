1. Create a usecase dir
mkdir /home/hduser/hiveusecase
copied custpayments_ORIG.sql and payments.txt into /home/hduser/hiveusecase location

2. Ensure Hadoop, MYSQL, Hive remote metastore is running.

Usecase 1:
**********

1. Login to Mysql and execute the sql file to load the custpayments table:
source /home/hduser/hiveusecase/custpayments_ORIG.sql

2. Write sqoop command to import data from customerpayments table with 2 mappers, with enclosed
by " (As we have ',' in the data itself we are importing in sqoop using --enclosed-by option into the
location /user/hduser/custpayments).

splitby col better choose with high cardinal col -> in this case customernumber is unique
and more distinct. analyze records in mysql customers table for high cardinal col.
sqoop import --connect jdbc:mysql://localhost/custpayments --username root --password root \
-table customers -m 2 --split-by customernumber --target-dir /user/hduser/custpayments \
--delete-target-dir --as-textfile --enclosed-by '\"';

(or) since the source table customers in mysql has primay key as customernumber we no need to specify
the split-by colname as customernumber also default fileformat will be textfile
no need to specify as-textfile
sqoop import --connect jdbc:mysql://localhost/custpayments --username root --password root \
-table customers -m 2 --target-dir /user/hduser/custpayments \
--delete-target-dir --enclosed-by '\"';

3. Create a hive external table and load the sqoop imported data to the hive table called custpayments.
As we have ',' in the data itself we are using quotedchar option below with the csv serde option as given
below as example, create the table with all columns.

create external table custmaster (customerNumber int,customername string,contactlastname string,
contactfirstname string,phone string,addressline1 string,addressline2 string,city string,state string,
postalcode string,country string,salesrepemployeenumber int,creditlimit float)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
"separatorChar" = ",",
"quoteChar" = "\"")
LOCATION '/user/hduser/custpayments/';

4. Copy the payments.txt into hdfs location /user/hduser/paymentsdata/ and Create an external table
namely payments with customernumber, checknumber, paymentdate, amount columns to point the
imported payments data.

hadoop fs -copyFromLocal -f /home/hduser/hiveusecase/payments.txt /user/hduser/paymentsdata/
(or)
hadoop fs -put -f /home/hduser/hiveusecase/payments.txt /user/hduser/paymentsdata/

create external table payments(customernumber int,checknumber string, paymentdate string,amount float)
row format delimited
fields terminated by ','
lines terminated by '\n'
stored as textfile
location '/user/hduser/paymentsdata/';

5. Create an external table called cust_payments in avro format and load data by doing inner join of
custmaster and payments tables, using insert select customernumber,
contactfirstname,contactlastname,phone, creditlimit from custmaster and paymentdate, amount
columns from payments table

create external table cust_payments(customernumber int,contactfirstname string,
contactlastname string,phone string,creditlimit float, paymentdate string, amount float)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as avro;

insert into table cust_payments select c.customernumber,c.contactfirstname,c.contactlastname,c.phone,c.creditlimit,p.paymentdate,p.amount
from custmaster as c inner join payments as p on c.customernumber = p.customernumber;
(or) we no need the datatype conversion(casting)
insert into table select int(c.customernumber),c.contactfirstname,c.contactlastname,c.phone,float(c.creditlimit),
p.paymentdate,float(p.amount) from custmaster as c inner join payments as p
on c.customernumber = p.customernumber;

6. Create a view called custpayments_vw to only display customernumber,creditlimit,paymentdate and
amount selected from cust_payments.

create view custpayments_vw as select customernumber,creditlimit,paymentdate,amount from cust_payments;


7. Extract only customernumber,creditlimit,paymentdate and amount columns either using the above
view/cust_payments table into hdfs location /user/hduser/custpaymentsexport with '|' delimiter.

create external table custpaymentsexport(customernumber int,creditlimit float,paymentdate string,amount float)
row format delimited fields terminated by '|'
lines terminated by '\n'
stored as textfile
location '/user/hduser/custpaymentsexport';

insert into table custpaymentsexport select customernumber,creditlimit,paymentdate,amount from custpayments_vw;

(or)
insert overwrite local directory '/home/hduser/custpaymentsexport'
row format delimited fields terminated by '|'
select customernumber,creditlimit,paymentdate,amount from custpayments_vw;

hadoop fs -copyFromLocal -f /home/hduser/custpaymentsexport/* /user/hduser/custpaymentsexport


8. Export the data from the /user/hduser/custpaymentsexport location to mysql table called
cust_payments using sqoop export with staging table option using records per statement 100 and
mappers 3.

first create main table and staging table in mysql
create table cust_payments(customernumber int(11),creditlimit decimal(10,2),paymentdate date,amount float);
create table custpaymentsexp_stage select * from cust_payments where 1=0;

sqoop export -Dsqoop.export.records.per.statement=100 \
--connect jdbc:mysql://localhost/custpayments --username root --password root \
--table cust_payments --export-dir custpaymentsexport \
--fields-terminated-by '|' --lines-terminated-by '\n' --batch \
--staging-table custpaymentsexp_stage \
--clear-staging-table --columns customernumber,creditlimit,paymentdate,amount -m 3;

Usecase 2:
Managing Fixed Width Data:
1. Copy the below fixed data into a linux file, load into a hive table called cust_fixed_raw in a column
rawdata.

1 Lara        chennai   55 2016-09-2110000
2 vasudevan   banglore  43 2016-09-2390000
3 Paul        chennai   33 2019-02-2020000
4 David Hanna New Jersey29 2019-04-22

copied above data to cust_fixed_raw.txt
vi cust_fixed_raw.txt

create table cust_fixed_raw(rawdata string);
load data local inpath '/home/hduser/cust_fixed_raw.txt' overwrite into table cust_fixed_raw;

2. Create a temporary table called cust_delimited_parsed_temp with columns such as
id,name,city,age,dt,amt and load the cust_fixed_raw table using substr.

for eg to select id column : select trim(substr(rawdata,1,3)) from cust_fixed_raw;

create table cust_delimited_parsed_temp(id int,name string,city string,age int,dt string,amt int)
row format delimited fields terminated by ','
stored as textfile;

insert into table cust_delimited_parsed_temp select trim(substr(rawdata,1,2)),
trim(substr(rawdata,3,12)),trim(substr(rawdata,15,10)),trim(substr(rawdata,25,3)),
trim(substr(rawdata,28,10)),CASE WHEN LENGTH(trim(substr(rawdata,38,5))) > 0
THEN trim(substr(rawdata,38,5)) ELSE 0 END
from cust_fixed_raw;


3. Export only id, dt and amt column into a mysql table cust_fixed_mysql using sqoop export.

->first create table cust_fixed_mysql in mysql
create table cust_fixed_mysql(id int,dt date,amt int);

->create a temp view or table with id int,dt date,amt int attributes
create view cust_delimited_parsed_temp_view as
select id, dt, amt from cust_delimited_parsed_temp;

->create other table using the above view
create external table cust_to_mysql(id int,dt string,amt int)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile location '/user/hduser/cust_to_mysql';
insert into cust_to_mysql select * from cust_delimited_parsed_temp_view;

sqoop export --connect "jdbc:mysql://localhost/custpayments" --username root --password root \
--table cust_fixed_mysql --export-dir cust_to_mysql --fields-terminated-by ',' ;

4. Load only chennai data to another table called cust_parsed_orc of type orc format partitioned based
on dt.

-since we used partitioned by don't include partition column while creating a table
create table cust_parsed_orc(id int,name string,city string,age int,amt int)
partitioned by(dt string)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as orcfile;

-in dynamic partition specify partition col dt at the last
where as in static partition no need to specify the partition col in insert select statement
insert into cust_parsed_orc partition(dt)
select id,name,city,age,amt,dt from cust_delimited_parsed_temp where city = 'chennai';

5. Create a json table called cust_parsed_json (to load into json use the following steps).

cd /home/hduser/hiveusecase

wget https://repo1.maven.org/maven2/org/apache/hive/hcatalog/hive-hcatalog-core/1.2.1/hive-hcatalog-core-1.2.1.jar

-in hive terminal
add jar /home/hduser/hiveusecase/hive-hcatalog-core-1.2.1.jar;
create external table cust_parsed_json(id int, name string,city string, age int)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
stored as textfile
location '/user/hduser/custjson';

to verify added jars in hive
>list jars

6. Insert into the cust_parsed_json only non chennai data using insert select of id,name,city, age from
the cust_delimited_parsed_temp table.

insert into cust_parsed_json select id,name,city,age from cust_delimited_parsed_temp
where city!='chennai';

7. Schema migration:
Convert the XML table called xml_bank created in the actual usecase to JSON data by the same way like
step 5 using create table as select.

For eg:
create external table xml_json
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
stored as textfile
location '/user/hduser/custxmljson'
as select * from xml_bank;
Solution:
add jar /home/hduser/hiveusecase/hivexmlserde-1.0.5.3.jar;
create external table xml_json(customer_id string, income bigint,demographics map<string,string>, financial map<string,string>)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
stored as textfile
location '/user/hduser/custxmljson';

insert into xml_json select * from xml_bank;
(or)
- we can create only managed tables with CTAS
create table xml_json2
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
stored as textfile
location '/user/hduser/custxmljson2'
as select * from xml_bank;




8. Import data from mysql directly creating static partition based on city=chennai as given below for
additional knowledge.

sqoop import \
--connect jdbc:mysql://localhost:3306/custdb \
--username root \
--password root \
--query "select custid,firstname,age from customer where city='chennai' and \$CONDITIONS" \
--target-dir /user/hduser/hiveext \
--split-by custid \
--hive-overwrite \
--hive-import \
--create-hive-table \
--hive-partition-key city \
--hive-partition-value 'chennai' \
--fields-terminated-by ',' \
--hive-table default.custinfo \
--direct